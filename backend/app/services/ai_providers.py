import openai
import anthropic
from google import genai
from typing import Dict, List, Any, AsyncGenerator, Union
import asyncio
import logging
import json
import os
import httpx
from .web_search_service import WebSearchService

logger = logging.getLogger(__name__)

class AIProviderService:
    def __init__(self):
        # è®¾ç½®ä¸åŒæ“ä½œçš„è¶…æ—¶æ—¶é—´
        self.default_timeout = 60  # é»˜è®¤60ç§’è¶…æ—¶
        self.responses_api_timeout = 180  # Responses API ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        self.config_timeout = 30  # é…ç½®åŠ è½½è¶…æ—¶
        self.max_retries = 2  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self._models_config = None
        self.web_search_service = WebSearchService()  # åˆå§‹åŒ–æœç´¢æœåŠ¡
        
    def _get_message_attr(self, msg: Union[Dict[str, Any], Any], attr: str) -> str:
        """å®‰å…¨åœ°è·å–æ¶ˆæ¯å±æ€§ï¼Œæ”¯æŒå­—å…¸å’ŒPydanticå¯¹è±¡"""
        if isinstance(msg, dict):
            return msg.get(attr, "")
        else:
            # Pydanticå¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®
            return getattr(msg, attr, "")
    
    def _convert_message_to_openai_format(self, msg: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """å°†æ¶ˆæ¯è½¬æ¢ä¸ºOpenAI APIæ ¼å¼ï¼Œæ”¯æŒå›¾ç‰‡å’Œæ–‡ä»¶é™„ä»¶"""
        role = self._get_message_attr(msg, "role")
        content = self._get_message_attr(msg, "content")
        
        # è·å–å›¾ç‰‡æ•°æ®
        images = None
        if isinstance(msg, dict):
            images = msg.get("images")
        else:
            images = getattr(msg, "images", None)
        
        # è·å–æ–‡ä»¶æ•°æ®
        files = None
        if isinstance(msg, dict):
            files = msg.get("files")
        else:
            files = getattr(msg, "files", None)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåª’ä½“å†…å®¹
        has_multimedia = (images and len(images) > 0) or (files and len(files) > 0)
        
        # å¦‚æœæ²¡æœ‰å¤šåª’ä½“å†…å®¹ï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
        if not has_multimedia:
            return {"role": role, "content": content}
        
        # æ„é€ æ”¯æŒå¤šåª’ä½“çš„æ¶ˆæ¯æ ¼å¼
        content_parts = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if content and content.strip():
            content_parts.append({"type": "text", "text": content})
        
        # æ·»åŠ å›¾ç‰‡å†…å®¹
        if images:
            for image in images:
                if isinstance(image, dict):
                    image_data = image.get("data")
                    mime_type = image.get("mime_type", "image/jpeg")
                else:
                    image_data = getattr(image, "data", "")
                    mime_type = getattr(image, "mime_type", "image/jpeg")
                
                if image_data:
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{image_data}"
                        }
                    })
        
        # æ·»åŠ æ–‡ä»¶å†…å®¹ (ä½¿ç”¨input_fileæ ¼å¼)
        if files:
            for file in files:
                if isinstance(file, dict):
                    file_id = file.get("openai_file_id")
                    filename = file.get("filename")
                    process_mode = file.get("process_mode", "direct")
                else:
                    file_id = getattr(file, "openai_file_id", "")
                    filename = getattr(file, "filename", "")
                    process_mode = getattr(file, "process_mode", "direct")
                
                if file_id:
                    # æ ¹æ®å¤„ç†æ¨¡å¼å†³å®šå¦‚ä½•å¤„ç†æ–‡ä»¶
                    if process_mode == "direct":
                        # ç›´è¯»æ¨¡å¼ï¼šä½¿ç”¨ input_file
                        content_parts.append({
                            "type": "input_file",
                            "file_id": file_id
                        })
                    # Code Interpreter å’Œ File Search æ¨¡å¼çš„æ–‡ä»¶ä¼šåœ¨å·¥å…·é…ç½®ä¸­å¤„ç†
        
        return {
            "role": role,
            "content": content_parts
        }
    
    def _prepare_tools_config(self, messages: List[Union[Dict[str, Any], Any]], tools: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å‡†å¤‡å·¥å…·é…ç½®ï¼ŒåŸºäºæ¶ˆæ¯ä¸­çš„æ–‡ä»¶ç±»å‹"""
        tools_config = {"tools": []}
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å·¥å…·
        need_code_interpreter = False
        need_file_search = False
        vector_stores = set()
        
        for msg in messages:
            # è·å–æ–‡ä»¶æ•°æ®
            files = None
            if isinstance(msg, dict):
                files = msg.get("files")
            else:
                files = getattr(msg, "files", None)
            
            if files:
                for file in files:
                    if isinstance(file, dict):
                        process_mode = file.get("process_mode", "direct")
                        vector_store_id = file.get("vector_store_id")
                    else:
                        process_mode = getattr(file, "process_mode", "direct")
                        vector_store_id = getattr(file, "vector_store_id", None)
                    
                    if process_mode == "code_interpreter":
                        need_code_interpreter = True
                    elif process_mode == "file_search" and vector_store_id:
                        need_file_search = True
                        vector_stores.add(vector_store_id)
        
        # æ·»åŠ éœ€è¦çš„å·¥å…·
        if need_code_interpreter:
            tools_config["tools"].append({
                "type": "code_interpreter",
                "container": {"type": "auto"}
            })
        
        if need_file_search and vector_stores:
            tools_config["tools"].append({
                "type": "file_search",
                "vector_store_ids": list(vector_stores)
            })
        
        # æ·»åŠ å‰ç«¯ä¼ æ¥çš„å·¥å…·é…ç½®ï¼ˆåŒ…æ‹¬ Web Search å’Œ Image Generationï¼‰
        if tools:
            for tool_config in tools:
                if tool_config.get("type") in ["web_search", "web_search_preview"]:
                    # æ„å»º web search å·¥å…·é…ç½®
                    web_search_tool = self.web_search_service.build_web_search_tool_config(tool_config)
                    tools_config["tools"].append(web_search_tool)
                elif tool_config.get("type") == "image_generation":
                    # æ„å»º image generation å·¥å…·é…ç½®
                    image_gen_tool = self._build_image_generation_tool_config(tool_config)
                    tools_config["tools"].append(image_gen_tool)
        
        return tools_config
    
    def _build_image_generation_tool_config(self, tool_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºå›¾ç‰‡ç”Ÿæˆå·¥å…·é…ç½®"""
        config = {
            "type": "image_generation"
        }
        
        # æ·»åŠ å¯é€‰çš„å›¾ç‰‡ç”Ÿæˆå‚æ•°
        if "size" in tool_config:
            config["size"] = tool_config["size"]
        
        if "quality" in tool_config:
            config["quality"] = tool_config["quality"]
        
        if "format" in tool_config:
            config["format"] = tool_config["format"]
        
        if "compression" in tool_config:
            config["compression"] = tool_config["compression"]
        
        if "background" in tool_config:
            config["background"] = tool_config["background"]
        
        if "input_fidelity" in tool_config:
            config["input_fidelity"] = tool_config["input_fidelity"]
        
        if "input_image" in tool_config:
            config["input_image"] = tool_config["input_image"]
        
        if "input_image_mask" in tool_config:
            config["input_image_mask"] = tool_config["input_image_mask"]
        
        if "partial_images" in tool_config:
            config["partial_images"] = tool_config["partial_images"]
        
        # é»˜è®¤è®¾ç½®å®¡æ ¸çº§åˆ«ä¸ºlowï¼ˆå¦‚ç”¨æˆ·è¦æ±‚ï¼‰
        config["moderation"] = tool_config.get("moderation", "low")
        
        return config
    
    def _find_previous_image_generation(self, messages: List[Union[Dict[str, Any], Any]]) -> str:
        """æŸ¥æ‰¾ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœçš„IDï¼Œç”¨äºå¤šè½®å›¾åƒç”Ÿæˆ"""
        # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹å‘åæŸ¥æ‰¾
        for msg in reversed(messages):
            role = self._get_message_attr(msg, "role")
            if role == "assistant":
                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆç»“æœ
                image_generations = None
                if isinstance(msg, dict):
                    image_generations = msg.get("image_generations")
                else:
                    image_generations = getattr(msg, "image_generations", None)
                
                if image_generations and len(image_generations) > 0:
                    # è¿”å›æœ€è¿‘çš„å›¾ç‰‡ç”ŸæˆID
                    return image_generations[-1].get("id", "")
        
        return ""
    
    async def _handle_web_search_fallback(
        self, 
        completion_params: Dict[str, Any], 
        tools: List[Dict[str, Any]], 
        messages: List[Union[Dict[str, str], Any]]
    ) -> Dict[str, Any]:
        """
        å¤„ç†Web Searchçš„æ—§ç‰ˆå›é€€é€»è¾‘
        ä½¿ç”¨æ—§ç‰ˆ web_search_preview å‚æ•°æ ¼å¼
        """
        # æŸ¥æ‰¾æœç´¢å·¥å…·é…ç½®
        search_tool = None
        for tool in tools:
            if tool.get("type") in ["web_search", "web_search_preview"]:
                search_tool = tool
                break
        
        if search_tool:
            # æ—§ç‰ˆå®ç°ï¼šæ·»åŠ  web_search_preview å‚æ•°
            completion_params["web_search_preview"] = True
            
            # æ·»åŠ ç”¨æˆ·ä½ç½®ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if search_tool.get("user_location"):
                completion_params["user_location"] = search_tool["user_location"]
        
        return completion_params
        
    async def _load_models_config(self) -> Dict[str, Any]:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        if self._models_config is None:
            config_url = "https://raw.githubusercontent.com/marvinli001/MineChatWeb/main/models-config.json"
            async with httpx.AsyncClient(timeout=self.config_timeout) as client:
                response = await client.get(config_url)
                response.raise_for_status()
                self._models_config = response.json()
                logger.info("æˆåŠŸä»è¿œç¨‹åŠ è½½æ¨¡å‹é…ç½®")
        return self._models_config
        
    async def get_completion(
        self,
        provider: str,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        stream: bool = False,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        reasoning: str = "medium",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None,
        base_url: str = None
    ) -> Dict[str, Any]:
        """è·å–AIå®Œæˆå“åº”"""
        logger.info(f"å¼€å§‹è°ƒç”¨ {provider} API, æ¨¡å‹: {model}, æ€è€ƒæ¨¡å¼: {thinking_mode}")
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶
        last_exception = None
        for attempt in range(self.max_retries + 1):
            try:
                if provider == "openai":
                    # OpenAI æä¾›å•†ç°åœ¨åªä½¿ç”¨ Responses API
                    return await self._openai_responses_completion(model, messages, api_key, thinking_mode, reasoning_summaries, reasoning, tools, use_native_search)
                elif provider == "anthropic":
                    return await self._anthropic_completion(model, messages, api_key, thinking_mode, tools, stream)
                elif provider == "google":
                    return await self._google_completion(model, messages, api_key, thinking_mode)
                elif provider == "openai_compatible":
                    return await self._openai_compatible_completion(model, messages, api_key, stream, thinking_mode, reasoning_summaries, tools, use_native_search, base_url)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
                    
            except asyncio.TimeoutError as e:
                last_exception = e
                logger.warning(f"{provider} APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries + 1})")
                if attempt < self.max_retries:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    logger.error(f"{provider} APIè°ƒç”¨åœ¨ {self.max_retries + 1} æ¬¡å°è¯•åä»ç„¶è¶…æ—¶")
                    raise Exception(f"{provider} APIè°ƒç”¨è¶…æ—¶ï¼Œå·²é‡è¯•{self.max_retries}æ¬¡ï¼Œè¯·ç¨åé‡è¯•")
            except Exception as e:
                last_exception = e
                # å¯¹äºæŸäº›é”™è¯¯ç±»å‹ï¼Œä¸è¿›è¡Œé‡è¯•
                if any(keyword in str(e).lower() for keyword in ['authentication', 'authorization', 'api key', 'invalid']):
                    logger.error(f"{provider} APIè°ƒç”¨å¤±è´¥ (è®¤è¯é”™è¯¯): {str(e)}")
                    raise
                elif attempt < self.max_retries:
                    logger.warning(f"{provider} APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {str(e)}")
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    logger.error(f"{provider} APIè°ƒç”¨åœ¨ {self.max_retries + 1} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {str(e)}")
                    raise
        
        # è¿™è¡Œä¸åº”è¯¥åˆ°è¾¾ï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨
        if last_exception:
            raise last_exception

    def _is_thinking_model(self, model: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹"""
        thinking_models = [
            'o1', 'o1-preview', 'o1-mini', 'o1-pro',
            'o3', 'o3-mini', 'o3-pro',
            'o4-mini', 'o4-mini-high'
        ]
        return model in thinking_models

    async def _is_openai_responses_api(self, model: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸º OpenAI Responses API æ¨¡å‹"""
        try:
            config = await self._load_models_config()  # æ·»åŠ  await
            openai_models = config.get('providers', {}).get('openai', {}).get('models', {})
            model_config = openai_models.get(model, {})
            return model_config.get('api_type') == 'responses'
        except Exception as e:
            logger.warning(f"æ— æ³•æ£€æŸ¥æ¨¡å‹APIç±»å‹: {e}")
            # å›é€€åˆ°ç¡¬ç¼–ç åˆ—è¡¨
            fallback_models = [
                'chatgpt-4o-latest',
                'gpt-4o-realtime-preview',
                'gpt-4o-realtime-preview-2024-10-01',
                'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-chat-latest',
                'gpt-4o', 'gpt-4o-mini',
                'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano',
                'o1', 'o1-preview', 'o1-mini', 'o3', 'o3-mini', 'o4-mini'
            ]
            return model in fallback_models

    async def _supports_streaming(self, provider: str, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º"""
        try:
            config = await self._load_models_config()  # æ·»åŠ  await
            provider_models = config.get('providers', {}).get(provider, {}).get('models', {})
            model_config = provider_models.get(model, {})
            return model_config.get('supports_streaming', False)
        except Exception as e:
            logger.warning(f"æ— æ³•æ£€æŸ¥æ¨¡å‹æµå¼æ”¯æŒ: {e}")
            # å¯¹äºOpenAIï¼Œé™¤äº†thinkingæ¨¡å‹å¤–ï¼Œé»˜è®¤æ”¯æŒæµå¼
            if provider == 'openai':
                thinking_models = ['o1', 'o1-preview', 'o1-mini', 'o3', 'o3-mini', 'o4-mini']
                return model not in thinking_models
            return False

    def _is_gpt5_model(self, model: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸º GPT-5 ç³»åˆ—æ¨¡å‹"""
        gpt5_models = ['gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-5-chat-latest']
        return model in gpt5_models

    def _supports_thinking_mode(self, model: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒ thinking mode (é€šè¿‡ reasoning_effort å‚æ•°)"""
        return self._is_gpt5_model(model)

    def _convert_responses_to_chat_format(self, responses_result: Dict[str, Any]) -> Dict[str, Any]:
        """å°† Responses API æ ¼å¼è½¬æ¢ä¸ºæ ‡å‡† Chat Completions æ ¼å¼"""
        if "choices" in responses_result:
            # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
            return responses_result
        
        if "output" not in responses_result:
            # ä¸æ˜¯ Responses API æ ¼å¼ï¼Œç›´æ¥è¿”å›
            return responses_result
        
        # è½¬æ¢ Responses API æ ¼å¼
        output = responses_result.get("output", [])
        choices = []
        reasoning_content = ""
        image_generations = []
        
        for item in output:
            # æå–æ¨ç†å†…å®¹
            if item.get("type") == "reasoning":
                summary_items = item.get("summary", [])
                for summary_item in summary_items:
                    if summary_item.get("type") == "summary_text":
                        reasoning_content = summary_item.get("text", "")
                        break
            
            # æå–å›¾ç‰‡ç”Ÿæˆç»“æœ
            elif item.get("type") == "image_generation_call":
                image_gen = {
                    "id": item.get("id"),
                    "type": "image_generation_call",
                    "status": item.get("status"),
                    "result": item.get("result"),  # base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
                    "revised_prompt": item.get("revised_prompt")
                }
                image_generations.append(image_gen)
            
            # æå–åŠ©æ‰‹æ¶ˆæ¯å†…å®¹
            elif item.get("type") == "message" and item.get("role") == "assistant":
                content = ""
                message_content = item.get("content", [])
                
                # æå–æ–‡æœ¬å†…å®¹
                for content_item in message_content:
                    if content_item.get("type") == "output_text":
                        content = content_item.get("text", "")
                        break
                
                # æ„é€ é€‰æ‹©å¯¹è±¡
                choice = {
                    "message": {
                        "role": "assistant",
                        "content": content
                    },
                    "finish_reason": "stop",
                    "index": 0
                }
                
                # å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                if reasoning_content:
                    choice["message"]["reasoning"] = reasoning_content
                
                # å¦‚æœæœ‰å›¾ç‰‡ç”Ÿæˆç»“æœï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                if image_generations:
                    choice["message"]["image_generations"] = image_generations
                
                choices.append(choice)
        
        # å¦‚æœæ²¡æœ‰åŠ©æ‰‹æ¶ˆæ¯ä½†æœ‰å›¾ç‰‡ç”Ÿæˆç»“æœï¼Œåˆ›å»ºä¸€ä¸ªç©ºæ¶ˆæ¯æ¥æ‰¿è½½å›¾ç‰‡
        if not choices and image_generations:
            choice = {
                "message": {
                    "role": "assistant",
                    "content": "",
                    "image_generations": image_generations
                },
                "finish_reason": "stop",
                "index": 0
            }
            choices.append(choice)
        
        # æ„é€ æ ‡å‡†æ ¼å¼å“åº”
        converted_result = {
            "id": responses_result.get("id", f"resp_{hash(str(output)) % 1000000}"),
            "choices": choices,
            "usage": responses_result.get("usage", {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            })
        }
        
        return converted_result

    async def _openai_chat_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        stream: bool = False,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None
    ) -> Dict[str, Any]:
        """OpenAI Chat Completions API è°ƒç”¨"""
        try:
            client = openai.AsyncOpenAI(
                api_key=api_key,
                timeout=self.default_timeout
            )
            
            logger.info(f"è°ƒç”¨OpenAIæ¨¡å‹: {model}, æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥æ”¯æŒå›¾ç‰‡å’Œæ–‡ä»¶
            converted_messages = [self._convert_message_to_openai_format(msg) for msg in messages]
            
            # å‡†å¤‡å·¥å…·é…ç½®
            tools_config = self._prepare_tools_config(messages, tools)
            
            # æ€è€ƒæ¨¡å‹ç‰¹æ®Šå¤„ç†
            if self._is_thinking_model(model):
                # å¯¹äºæ€è€ƒæ¨¡å‹ï¼Œè¿‡æ»¤æ‰systemæ¶ˆæ¯å¹¶æ·»åŠ reasoning_summarieså‚æ•°
                filtered_messages = [msg for msg in converted_messages if msg.get("role") != "system"]
                logger.info(f"æ€è€ƒæ¨¡å‹ {model} è¿‡æ»¤åæ¶ˆæ¯æ•°é‡: {len(filtered_messages)}")
                
                completion_params = {
                    "model": model,
                    "messages": filtered_messages
                }
                
                # æ·»åŠ å·¥å…·é…ç½®
                if tools_config["tools"]:
                    completion_params.update(tools_config)
                
                # æ³¨æ„ï¼šreasoning_summaries å‚æ•°åœ¨å½“å‰ OpenAI API ç‰ˆæœ¬ä¸­å¯èƒ½ä¸è¢«æ”¯æŒ
                # å¦‚æœéœ€è¦æ”¯æŒè¯¥å‚æ•°ï¼Œè¯·æ£€æŸ¥ OpenAI API æ–‡æ¡£å’Œåº“ç‰ˆæœ¬
                # if reasoning_summaries and reasoning_summaries != "hide":
                #     completion_params["reasoning_summaries"] = reasoning_summaries
                
                response = await asyncio.wait_for(
                    client.chat.completions.create(**completion_params),
                    timeout=self.default_timeout
                )
            else:
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„å‚æ•°
                completion_params = {
                    "model": model,
                    "messages": converted_messages,
                    "stream": stream
                }
                
                # å¤„ç†Web Searchå·¥å…·çš„å›é€€é€»è¾‘
                if tools and use_native_search is False:
                    # å¯¹äºä¸æ”¯æŒæ–°ç‰ˆweb_searchçš„æ¨¡å‹ï¼Œä½¿ç”¨æ—§ç‰ˆå›é€€
                    completion_params = await self._handle_web_search_fallback(
                        completion_params, tools, messages
                    )
                elif tools_config["tools"]:
                    completion_params.update(tools_config)
                
                # GPT-5 ç³»åˆ—æ¨¡å‹ä¸æ”¯æŒè‡ªå®šä¹‰ temperatureï¼Œä½¿ç”¨é»˜è®¤å€¼ 1
                if not self._is_gpt5_model(model):
                    completion_params["temperature"] = 0.7
                
                # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ max_tokens
                if self._is_gpt5_model(model):
                    completion_params["max_completion_tokens"] = 4000
                else:
                    completion_params["max_tokens"] = 4000
                
                response = await asyncio.wait_for(
                    client.chat.completions.create(**completion_params),
                    timeout=self.default_timeout
                )
            
            result = response.model_dump()
            logger.info(f"OpenAI APIè°ƒç”¨æˆåŠŸï¼Œè¿”å›é€‰æ‹©æ•°é‡: {len(result.get('choices', []))}")
            return result
            
        except openai.AuthenticationError as e:
            logger.error(f"OpenAIè®¤è¯å¤±è´¥: {str(e)}")
            raise Exception("OpenAI APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥")
        except openai.RateLimitError as e:
            logger.error(f"OpenAIé€Ÿç‡é™åˆ¶: {str(e)}")
            raise Exception("OpenAI APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•")
        except openai.InternalServerError as e:
            logger.error(f"OpenAIæœåŠ¡å™¨é”™è¯¯: {str(e)}")
            raise Exception("OpenAIæœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")

    async def _openai_responses_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        reasoning: str = "medium",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None
    ) -> Dict[str, Any]:
        """OpenAI Responses API è°ƒç”¨"""
        try:
            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸º Responses API é€šå¸¸éœ€è¦æ›´å¤šæ—¶é—´
            timeout = self.responses_api_timeout if self._is_gpt5_model(model) or thinking_mode else self.default_timeout
            client = openai.AsyncOpenAI(
                api_key=api_key,
                timeout=timeout
            )
            
            logger.info(f"è°ƒç”¨OpenAI Responses APIæ¨¡å‹: {model}, æ€è€ƒæ¨¡å¼: {thinking_mode}")
            
            # å¯¹äº GPT-5 ç³»åˆ—æ¨¡å‹ï¼Œä½¿ç”¨ Responses API æ”¯æŒ thinking mode
            if self._is_gpt5_model(model) and thinking_mode:
                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æ¶ˆæ¯
                has_images = any(
                    (isinstance(msg, dict) and msg.get("images")) or
                    (hasattr(msg, "images") and getattr(msg, "images", None))
                    for msg in messages
                )
                
                if has_images:
                    # Responses API æ”¯æŒå›¾ç‰‡ï¼Œéœ€è¦ä½¿ç”¨æ–°çš„æ ¼å¼
                    logger.info(f"æ£€æµ‹åˆ°å›¾ç‰‡æ¶ˆæ¯ï¼Œä½¿ç”¨Responses APIçš„å¤šæ¨¡æ€è¾“å…¥æ ¼å¼")
                    
                    # è½¬æ¢æ¶ˆæ¯ä¸º Responses API æ ¼å¼
                    input_messages = []
                    instructions_text = ""
                    
                    for msg in messages:
                        role = self._get_message_attr(msg, "role")
                        content = self._get_message_attr(msg, "content")
                        
                        if role == "system":
                            instructions_text = content
                        else:
                            # è·å–å›¾ç‰‡å’Œæ–‡ä»¶æ•°æ®
                            images = None
                            files = None
                            if isinstance(msg, dict):
                                images = msg.get("images")
                                files = msg.get("files")
                            else:
                                images = getattr(msg, "images", None)
                                files = getattr(msg, "files", None)
                            
                            # æ„é€ å†…å®¹éƒ¨åˆ†æ•°ç»„
                            content_parts = []
                            
                            # æ·»åŠ æ–‡æœ¬å†…å®¹
                            if content and content.strip():
                                content_parts.append({"type": "input_text", "text": content})
                            
                            # æ·»åŠ å›¾ç‰‡å†…å®¹
                            if images and len(images) > 0:
                                for image in images:
                                    if isinstance(image, dict):
                                        image_data = image.get("data")
                                        mime_type = image.get("mime_type", "image/jpeg")
                                    else:
                                        image_data = getattr(image, "data", "")
                                        mime_type = getattr(image, "mime_type", "image/jpeg")
                                    
                                    if image_data:
                                        content_parts.append({
                                            "type": "input_image",
                                            "image_url": f"data:{mime_type};base64,{image_data}"
                                        })
                            
                            # æ·»åŠ æ–‡ä»¶å†…å®¹ï¼ˆä»…æ”¯æŒ direct æ¨¡å¼çš„ PDF æ–‡ä»¶ï¼‰
                            if files and len(files) > 0:
                                for file in files:
                                    if isinstance(file, dict):
                                        openai_file_id = file.get("openai_file_id")
                                        process_mode = file.get("process_mode", "direct")
                                    else:
                                        openai_file_id = getattr(file, "openai_file_id", None)
                                        process_mode = getattr(file, "process_mode", "direct")
                                    
                                    # åªæœ‰ direct æ¨¡å¼çš„æ–‡ä»¶æ‰æ·»åŠ åˆ° input_fileï¼ˆä»…æ”¯æŒ PDFï¼‰
                                    if openai_file_id and process_mode == "direct":
                                        content_parts.append({
                                            "type": "input_file",
                                            "file_id": openai_file_id
                                        })
                            
                            # å¦‚æœæ²¡æœ‰å†…å®¹éƒ¨åˆ†ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
                            if not content_parts:
                                content_parts.append({"type": "input_text", "text": ""})
                            
                            input_messages.append({
                                "role": role,
                                "content": content_parts
                            })
                    
                    # å‡†å¤‡å·¥å…·é…ç½®
                    tools_config = self._prepare_tools_config(messages, tools)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆå·¥å…·å¹¶æŸ¥æ‰¾ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœ
                    previous_image_gen_id = ""
                    has_image_gen_tool = tools and any(tool.get("type") == "image_generation" for tool in tools)
                    if has_image_gen_tool:
                        previous_image_gen_id = self._find_previous_image_generation(messages)
                    
                    # ä½¿ç”¨ Responses API çš„å¤šæ¨¡æ€å‚æ•°ç»“æ„
                    completion_params = {
                        "model": model,
                        "input": input_messages,
                        "reasoning": {
                            "effort": reasoning,
                            "summary": reasoning_summaries if reasoning_summaries != "auto" else "auto"
                        }
                    }
                    
                    # å¦‚æœæœ‰ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­ï¼ˆç”¨äºå¤šè½®å›¾åƒç”Ÿæˆï¼‰
                    if previous_image_gen_id:
                        completion_params["previous_response_id"] = previous_image_gen_id
                        logger.info(f"ä½¿ç”¨previous_response_idè¿›è¡Œå¤šè½®å›¾åƒç”Ÿæˆ: {previous_image_gen_id}")
                    
                    # æ·»åŠ å·¥å…·é…ç½®
                    if tools_config["tools"]:
                        completion_params.update(tools_config)
                        # å¦‚æœæœ‰å·¥å…·ä¸”æ˜¯å¿…éœ€çš„ï¼Œè®¾ç½® tool_choice
                        need_code_interpreter = any(tool.get("type") == "code_interpreter" for tool in tools_config["tools"])
                        if need_code_interpreter:
                            completion_params["tool_choice"] = "required"
                    
                    # æ·»åŠ  instructions å¦‚æœæœ‰ system æ¶ˆæ¯
                    if instructions_text:
                        completion_params["instructions"] = instructions_text
                    
                    # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_output_tokens
                    completion_params["max_output_tokens"] = 4000
                    
                    # æ‰“å°å®é™…å‘é€çš„ JSON
                    logger.info(f"ğŸ“¤ å‘é€ç»™ OpenAI Responses API çš„å®Œæ•´è¯·æ±‚: {json.dumps(completion_params, ensure_ascii=False, indent=2)}")
                    
                    response = await asyncio.wait_for(
                        client.responses.create(**completion_params),
                        timeout=timeout
                    )
                    
                    result = response.model_dump()
                    logger.info(f"OpenAI Responses APIè°ƒç”¨æˆåŠŸï¼ˆå¤šæ¨¡æ€æ”¯æŒï¼‰")
                    return self._convert_responses_to_chat_format(result)
                
                # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Responses API æ‰€éœ€çš„ input æ ¼å¼
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨ç»“æ„åŒ–è¾“å…¥æ ¼å¼
                has_files = any(
                    (isinstance(msg, dict) and msg.get("files")) or
                    (hasattr(msg, "files") and getattr(msg, "files", None))
                    for msg in messages
                )
                
                instructions_text = ""
                
                if has_files:
                    # ä½¿ç”¨ç»“æ„åŒ–è¾“å…¥æ ¼å¼æ”¯æŒæ–‡ä»¶
                    input_messages = []
                    
                    for msg in messages:
                        role = self._get_message_attr(msg, "role")
                        content = self._get_message_attr(msg, "content")
                        
                        if role == "system":
                            instructions_text = content
                        else:
                            # è·å–æ–‡ä»¶æ•°æ®
                            files = None
                            if isinstance(msg, dict):
                                files = msg.get("files")
                            else:
                                files = getattr(msg, "files", None)
                            
                            # æ„é€ å†…å®¹éƒ¨åˆ†æ•°ç»„
                            content_parts = []
                            
                            # æ·»åŠ æ–‡æœ¬å†…å®¹
                            if content and content.strip():
                                content_parts.append({"type": "input_text", "text": content})
                            
                            # æ·»åŠ æ–‡ä»¶å†…å®¹ï¼ˆä»…æ”¯æŒ direct æ¨¡å¼çš„ PDF æ–‡ä»¶ï¼‰
                            if files and len(files) > 0:
                                for file in files:
                                    if isinstance(file, dict):
                                        openai_file_id = file.get("openai_file_id")
                                        process_mode = file.get("process_mode", "direct")
                                    else:
                                        openai_file_id = getattr(file, "openai_file_id", None)
                                        process_mode = getattr(file, "process_mode", "direct")
                                    
                                    # åªæœ‰ direct æ¨¡å¼çš„æ–‡ä»¶æ‰æ·»åŠ åˆ° input_fileï¼ˆä»…æ”¯æŒ PDFï¼‰
                                    if openai_file_id and process_mode == "direct":
                                        content_parts.append({
                                            "type": "input_file",
                                            "file_id": openai_file_id
                                        })
                            
                            # å¦‚æœæ²¡æœ‰å†…å®¹éƒ¨åˆ†ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
                            if not content_parts:
                                content_parts.append({"type": "input_text", "text": ""})
                            
                            input_messages.append({
                                "role": role,
                                "content": content_parts
                            })
                    
                    # å‡†å¤‡å·¥å…·é…ç½®
                    tools_config = self._prepare_tools_config(messages, tools)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆå·¥å…·å¹¶æŸ¥æ‰¾ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœ
                    previous_image_gen_id = ""
                    has_image_gen_tool = tools and any(tool.get("type") == "image_generation" for tool in tools)
                    if has_image_gen_tool:
                        previous_image_gen_id = self._find_previous_image_generation(messages)
                    
                    # ä½¿ç”¨ç»“æ„åŒ–è¾“å…¥æ ¼å¼
                    completion_params = {
                        "model": model,
                        "input": input_messages,
                        "reasoning": {
                            "effort": reasoning,
                            "summary": reasoning_summaries if reasoning_summaries != "auto" else "auto"
                        }
                    }
                    
                    # å¦‚æœæœ‰ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­ï¼ˆç”¨äºå¤šè½®å›¾åƒç”Ÿæˆï¼‰
                    if previous_image_gen_id:
                        completion_params["previous_response_id"] = previous_image_gen_id
                        logger.info(f"ä½¿ç”¨previous_response_idè¿›è¡Œå¤šè½®å›¾åƒç”Ÿæˆ: {previous_image_gen_id}")
                    
                    # æ·»åŠ å·¥å…·é…ç½®
                    if tools_config["tools"]:
                        completion_params.update(tools_config)
                        # å¦‚æœæœ‰å·¥å…·ä¸”æ˜¯å¿…éœ€çš„ï¼Œè®¾ç½® tool_choice
                        need_code_interpreter = any(tool.get("type") == "code_interpreter" for tool in tools_config["tools"])
                        if need_code_interpreter:
                            completion_params["tool_choice"] = "required"
                    
                else:
                    # çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                    input_text = ""
                    
                    for msg in messages:
                        role = self._get_message_attr(msg, "role")
                        content = self._get_message_attr(msg, "content")
                        
                        if role == "system":
                            instructions_text = content
                        elif role == "user":
                            input_text += f"{content}\n"
                        elif role == "assistant":
                            input_text += f"Assistant: {content}\n"
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆå·¥å…·å¹¶æŸ¥æ‰¾ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœ
                    previous_image_gen_id = ""
                    has_image_gen_tool = tools and any(tool.get("type") == "image_generation" for tool in tools)
                    if has_image_gen_tool:
                        previous_image_gen_id = self._find_previous_image_generation(messages)
                    
                    completion_params = {
                        "model": model,
                        "input": input_text.strip(),
                        "reasoning": {
                            "effort": reasoning,
                            "summary": reasoning_summaries if reasoning_summaries != "auto" else "auto"
                        }
                    }
                    
                    # å¦‚æœæœ‰ä¹‹å‰çš„å›¾ç‰‡ç”Ÿæˆç»“æœï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­ï¼ˆç”¨äºå¤šè½®å›¾åƒç”Ÿæˆï¼‰
                    if previous_image_gen_id:
                        completion_params["previous_response_id"] = previous_image_gen_id
                        logger.info(f"ä½¿ç”¨previous_response_idè¿›è¡Œå¤šè½®å›¾åƒç”Ÿæˆ: {previous_image_gen_id}")
                
                # æ·»åŠ  instructions å¦‚æœæœ‰ system æ¶ˆæ¯
                if instructions_text:
                    completion_params["instructions"] = instructions_text
                
                # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_output_tokens (ä¸æ˜¯ max_completion_tokens)
                completion_params["max_output_tokens"] = 4000
                
                # å‡†å¤‡å·¥å…·é…ç½®å¹¶æ·»åŠ åˆ°è¯·æ±‚ä¸­
                tools_config = self._prepare_tools_config(messages, tools)
                if tools_config["tools"]:
                    completion_params.update(tools_config)
                
                # æ‰“å°å®é™…å‘é€çš„ JSON
                logger.info(f"ğŸ“¤ å‘é€ç»™ OpenAI Responses API çš„å®Œæ•´è¯·æ±‚: {json.dumps(completion_params, ensure_ascii=False, indent=2)}")
                logger.info(f"ä½¿ç”¨ Responses API å‚æ•°æ ¼å¼{'ï¼ˆåŒ…å«æ–‡ä»¶æ”¯æŒï¼‰' if has_files else 'ï¼ˆçº¯æ–‡æœ¬æ¨¡å¼ï¼‰'}")
                
                # è°ƒç”¨ Responses API
                try:
                    response = await asyncio.wait_for(
                        client.responses.create(**completion_params),
                        timeout=timeout
                    )
                except AttributeError:
                    # Fallback to chat completions if responses API not available
                    logger.warning("Responses API ä¸å¯ç”¨ï¼Œå›é€€åˆ° Chat Completions API")
                    # ä¸º chat completions é‡æ–°æ„é€ å‚æ•°
                    chat_params = {
                        "model": model,
                        "messages": messages,
                        "max_completion_tokens": 4000
                    }
                    response = await asyncio.wait_for(
                        client.chat.completions.create(**chat_params),
                        timeout=timeout
                    )
            else:
                # æ ‡å‡†çš„ Responses API è°ƒç”¨ï¼ˆå¯¹äºå…¶ä»–æ ‡è®°ä¸º responses çš„æ¨¡å‹ï¼‰
                completion_params = {
                    "model": model,
                    "messages": messages
                }
                
                # æ€è€ƒæ¨¡å‹å¤„ç†
                if self._is_thinking_model(model):
                    # è¿‡æ»¤systemæ¶ˆæ¯
                    filtered_messages = [msg for msg in messages if self._get_message_attr(msg, "role") != "system"]
                    completion_params["messages"] = filtered_messages
                
                # GPT-5 ç³»åˆ—æ¨¡å‹ä¸æ”¯æŒè‡ªå®šä¹‰ temperatureï¼Œä½¿ç”¨é»˜è®¤å€¼ 1
                if not self._is_gpt5_model(model):
                    completion_params["temperature"] = 0.7
                
                # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ max_tokens
                if self._is_gpt5_model(model):
                    completion_params["max_completion_tokens"] = 4000
                else:
                    completion_params["max_tokens"] = 4000
                
                # ä½¿ç”¨ Chat Completions API
                response = await asyncio.wait_for(
                    client.chat.completions.create(**completion_params),
                    timeout=timeout
                )
            
            result = response.model_dump()
            logger.info(f"OpenAI Responses APIè°ƒç”¨æˆåŠŸ")
            
            # è½¬æ¢ Responses API æ ¼å¼ä¸ºæ ‡å‡† Chat Completions æ ¼å¼
            converted_result = self._convert_responses_to_chat_format(result)
            return converted_result
            
        except Exception as e:
            logger.error(f"OpenAI Responses APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"OpenAI Responses APIè°ƒç”¨å¤±è´¥: {str(e)}")

    def _convert_message_to_anthropic_format(self, msg: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
        """å°†æ¶ˆæ¯è½¬æ¢ä¸ºAnthropic Messages APIæ ¼å¼ï¼Œæ”¯æŒå›¾ç‰‡ã€æ–‡ä»¶ã€æœç´¢ç»“æœå’Œå¼•ç”¨"""
        role = self._get_message_attr(msg, "role")
        content = self._get_message_attr(msg, "content")
        
        # è·å–å¤šåª’ä½“å’Œé™„åŠ æ•°æ®
        images = None
        files = None
        search_results = None
        citations_enabled = False
        
        if isinstance(msg, dict):
            images = msg.get("images")
            files = msg.get("files")
            search_results = msg.get("search_results")
            citations_enabled = msg.get("citations_enabled", False)
        else:
            images = getattr(msg, "images", None)
            files = getattr(msg, "files", None)
            search_results = getattr(msg, "search_results", None)
            citations_enabled = getattr(msg, "citations_enabled", False)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåª’ä½“å†…å®¹
        has_multimedia = (images and len(images) > 0) or (files and len(files) > 0) or (search_results and len(search_results) > 0)
        
        # å¦‚æœæ²¡æœ‰å¤šåª’ä½“å†…å®¹ï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
        if not has_multimedia:
            return {"role": role, "content": content}
        
        # æ„é€ æ”¯æŒå¤šåª’ä½“çš„æ¶ˆæ¯æ ¼å¼
        content_parts = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if content and content.strip():
            content_parts.append({"type": "text", "text": content})
        
        # æ·»åŠ å›¾ç‰‡å†…å®¹ï¼ˆVisionæ”¯æŒï¼‰
        if images:
            for image in images:
                if isinstance(image, dict):
                    image_data = image.get("data")
                    mime_type = image.get("mime_type", "image/jpeg")
                else:
                    image_data = getattr(image, "data", "")
                    mime_type = getattr(image, "mime_type", "image/jpeg")
                
                if image_data:
                    content_parts.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": image_data
                        }
                    })
        
        # æ·»åŠ æ–‡æ¡£å†…å®¹ï¼ˆFiles APIå’ŒPDFæ”¯æŒï¼‰
        if files:
            for file in files:
                if isinstance(file, dict):
                    file_id = file.get("anthropic_file_id")
                    filename = file.get("filename")
                    file_data = file.get("data")  # base64æ•°æ®
                    mime_type = file.get("mime_type", "application/pdf")
                else:
                    file_id = getattr(file, "anthropic_file_id", "")
                    filename = getattr(file, "filename", "")
                    file_data = getattr(file, "data", "")
                    mime_type = getattr(file, "mime_type", "application/pdf")
                
                if file_id:
                    # ä½¿ç”¨Files APIä¸Šä¼ çš„æ–‡ä»¶
                    content_parts.append({
                        "type": "document",
                        "source": {
                            "type": "file",
                            "file_id": file_id
                        },
                        "title": filename or "Document",
                        "citations": {"enabled": citations_enabled}
                    })
                elif file_data and mime_type == "application/pdf":
                    # ç›´æ¥ä¼ è¾“çš„PDFæ–‡ä»¶
                    content_parts.append({
                        "type": "document",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": file_data
                        },
                        "title": filename or "PDF Document",
                        "citations": {"enabled": citations_enabled}
                    })
                elif file_data and mime_type.startswith("text/"):
                    # æ–‡æœ¬æ–‡ä»¶
                    content_parts.append({
                        "type": "document",
                        "source": {
                            "type": "text",
                            "media_type": mime_type,
                            "data": file_data  # å‡è®¾æ˜¯æ–‡æœ¬å†…å®¹ï¼Œä¸æ˜¯base64
                        },
                        "title": filename or "Text Document",
                        "citations": {"enabled": citations_enabled}
                    })
        
        # æ·»åŠ æœç´¢ç»“æœå†…å®¹ï¼ˆSearch Resultsæ”¯æŒï¼‰
        if search_results:
            for result in search_results:
                if isinstance(result, dict):
                    source = result.get("source", "")
                    title = result.get("title", "")
                    result_content = result.get("content", "")
                else:
                    source = getattr(result, "source", "")
                    title = getattr(result, "title", "")
                    result_content = getattr(result, "content", "")
                
                if result_content:
                    content_parts.append({
                        "type": "search_result",
                        "source": source,
                        "title": title,
                        "content": [
                            {
                                "type": "text",
                                "text": result_content
                            }
                        ],
                        "citations": {"enabled": citations_enabled}
                    })
        
        return {
            "role": role,
            "content": content_parts
        }

    async def _anthropic_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False,
        tools: List[Dict[str, Any]] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """Anthropic Claude Messages API è°ƒç”¨ï¼Œæ”¯æŒExtended Thinkingã€Visionã€Filesã€Citationså’ŒSearch Results"""
        try:
            client = anthropic.AsyncAnthropic(
                api_key=api_key,
                timeout=self.default_timeout
            )
            
            logger.info(f"è°ƒç”¨Anthropicæ¨¡å‹: {model}, æ‰©å±•æ€è€ƒæ¨¡å¼: {thinking_mode}, æµå¼è¾“å‡º: {stream}")
            
            system_message = ""
            user_messages = []
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥æ”¯æŒå¤šåª’ä½“å†…å®¹
            for msg in messages:
                role = self._get_message_attr(msg, "role")
                content = self._get_message_attr(msg, "content")
                
                if role == "system":
                    system_message = content
                else:
                    # è½¬æ¢ä¸ºAnthropic Messages APIæ ¼å¼
                    converted_msg = self._convert_message_to_anthropic_format(msg)
                    user_messages.append(converted_msg)
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            kwargs = {
                "model": model,
                "max_tokens": 4000,
                "messages": user_messages,
                "temperature": 0.7,
                "stream": stream
            }
            
            # æ·»åŠ systemæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if system_message:
                kwargs["system"] = system_message
            
            # Extended Thinkingæ”¯æŒï¼ˆClaudeä¸å…è®¸ç”¨æˆ·è®¾ç½®budget_tokensï¼Œä½¿ç”¨å›ºå®šå€¼10000ï¼‰
            if thinking_mode:
                kwargs["thinking"] = {
                    "type": "enabled",
                    "budget_tokens": 10000  # å›ºå®šå€¼ï¼Œå¦‚æ–‡æ¡£è¦æ±‚
                }
                logger.info("å¯ç”¨Claudeæ‰©å±•æ€è€ƒæ¨¡å¼ï¼Œbudget_tokens: 10000")
            
            # å·¥å…·é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
            if tools:
                # è½¬æ¢å·¥å…·é…ç½®ä¸ºAnthropicæ ¼å¼
                anthropic_tools = self._convert_tools_to_anthropic_format(tools)
                if anthropic_tools:
                    kwargs["tools"] = anthropic_tools
            
            # è°ƒç”¨Anthropic Messages API
            response = await asyncio.wait_for(
                client.messages.create(**kwargs),
                timeout=self.default_timeout
            )
            
            # è½¬æ¢å“åº”ä¸ºOpenAIå…¼å®¹æ ¼å¼
            result = self._convert_anthropic_response_to_openai_format(response, thinking_mode)
            
            logger.info(f"Anthropic APIè°ƒç”¨æˆåŠŸï¼Œå“åº”å†…å®¹å—æ•°é‡: {len(response.content) if hasattr(response, 'content') else 0}")
            return result
            
        except anthropic.AuthenticationError as e:
            logger.error(f"Anthropicè®¤è¯å¤±è´¥: {str(e)}")
            raise Exception("Anthropic APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥")
        except anthropic.RateLimitError as e:
            logger.error(f"Anthropicé€Ÿç‡é™åˆ¶: {str(e)}")
            raise Exception("Anthropic APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•")
        except Exception as e:
            logger.error(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")

    def _convert_tools_to_anthropic_format(self, tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†å·¥å…·é…ç½®è½¬æ¢ä¸ºAnthropicæ ¼å¼"""
        anthropic_tools = []
        
        for tool in tools:
            tool_type = tool.get("type")
            
            if tool_type == "web_search" or tool_type == "web_search_20250305":
                # Web Searchå·¥å…· - æ”¯æŒAnthropic web_search_20250305æ ¼å¼
                anthropic_tool = {
                    "type": "web_search_20250305",
                    "name": "web_search"
                }
                
                # æ·»åŠ å¯é€‰å‚æ•°
                if tool.get("max_uses"):
                    anthropic_tool["max_uses"] = tool.get("max_uses", 5)
                
                if tool.get("user_location"):
                    anthropic_tool["user_location"] = tool.get("user_location")
                
                if tool.get("allowed_domains"):
                    anthropic_tool["allowed_domains"] = tool.get("allowed_domains")
                
                if tool.get("blocked_domains"):
                    anthropic_tool["blocked_domains"] = tool.get("blocked_domains")
                
                anthropic_tools.append(anthropic_tool)
            
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–å·¥å…·ç±»å‹çš„æ”¯æŒ
        
        return anthropic_tools

    def _convert_anthropic_response_to_openai_format(self, response: Any, thinking_mode: bool = False) -> Dict[str, Any]:
        """å°†Anthropicå“åº”è½¬æ¢ä¸ºOpenAIå…¼å®¹æ ¼å¼"""
        content_text = ""
        thinking_content = ""
        citations = []
        
        # å¤„ç†å“åº”å†…å®¹
        if hasattr(response, 'content') and response.content:
            for content_block in response.content:
                if hasattr(content_block, 'type'):
                    if content_block.type == "text":
                        # æ–‡æœ¬å†…å®¹
                        content_text += getattr(content_block, 'text', '')
                        
                        # æå–citationsï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(content_block, 'citations') and content_block.citations:
                            for citation in content_block.citations:
                                citations.append({
                                    "type": getattr(citation, 'type', 'unknown'),
                                    "cited_text": getattr(citation, 'cited_text', ''),
                                    "source": getattr(citation, 'source', ''),
                                    "title": getattr(citation, 'title', ''),
                                    "document_index": getattr(citation, 'document_index', 0),
                                    "start_char_index": getattr(citation, 'start_char_index', 0),
                                    "end_char_index": getattr(citation, 'end_char_index', 0)
                                })
                    
                    elif content_block.type == "thinking" and thinking_mode:
                        # æ‰©å±•æ€è€ƒå†…å®¹
                        thinking_content = getattr(content_block, 'content', '') or getattr(content_block, 'thinking', '')
        
        # æ„å»ºæ¶ˆæ¯å¯¹è±¡
        message = {
            "role": "assistant",
            "content": content_text
        }
        
        # æ·»åŠ æ€è€ƒå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if thinking_content and thinking_mode:
            message["reasoning"] = thinking_content
        
        # æ·»åŠ citationsï¼ˆå¦‚æœæœ‰ï¼‰
        if citations:
            message["citations"] = citations
        
        # æ„å»ºå®Œæ•´å“åº”
        result = {
            "id": f"msg_{getattr(response, 'id', 'unknown')}",
            "choices": [{
                "message": message,
                "finish_reason": self._map_anthropic_stop_reason(getattr(response, 'stop_reason', None))
            }],
            "usage": {
                "prompt_tokens": getattr(response.usage, 'input_tokens', 0) if hasattr(response, 'usage') else 0,
                "completion_tokens": getattr(response.usage, 'output_tokens', 0) if hasattr(response, 'usage') else 0,
                "total_tokens": (getattr(response.usage, 'input_tokens', 0) + getattr(response.usage, 'output_tokens', 0)) if hasattr(response, 'usage') else 0
            }
        }
        
        return result

    def _map_anthropic_stop_reason(self, stop_reason: str) -> str:
        """æ˜ å°„Anthropicçš„åœæ­¢åŸå› åˆ°OpenAIæ ¼å¼"""
        mapping = {
            "end_turn": "stop",
            "max_tokens": "length",
            "tool_use": "tool_calls"
        }
        return mapping.get(stop_reason, "stop")

    async def _google_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False
    ) -> Dict[str, Any]:
        """Google Gemini API è°ƒç”¨"""
        try:
            genai.configure(api_key=api_key)
            
            logger.info(f"è°ƒç”¨Googleæ¨¡å‹: {model}")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            history = []
            for msg in messages[:-1]:  # é™¤æœ€åä¸€æ¡æ¶ˆæ¯å¤–çš„å†å²
                role = self._get_message_attr(msg, "role")
                content = self._get_message_attr(msg, "content")
                
                if role == "user":
                    history.append({"role": "user", "parts": [content]})
                elif role == "assistant":
                    history.append({"role": "model", "parts": [content]})
            
            model_instance = genai.GenerativeModel(model)
            chat = model_instance.start_chat(history=history)
            
            # å‘é€æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = self._get_message_attr(messages[-1], "content")
            response = await asyncio.wait_for(
                chat.send_message_async(user_message),
                timeout=self.default_timeout
            )
            
            # è½¬æ¢ä¸ºOpenAIæ ¼å¼
            result = {
                "id": f"gemini_{hash(response.text) % 1000000}",
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": response.text
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,  # Google APIä¸æä¾›tokenè®¡æ•°
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
            
            logger.info(f"Google APIè°ƒç”¨æˆåŠŸ")
            return result
            
        except Exception as e:
            logger.error(f"Google APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Google APIè°ƒç”¨å¤±è´¥: {str(e)}")

    async def _openai_compatible_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        stream: bool = False,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None,
        base_url: str = None
    ) -> Dict[str, Any]:
        """OpenAI å…¼å®¹ API è°ƒç”¨ (Chat Completions API)"""
        try:
            # å¦‚æœæ²¡æœ‰æä¾›base_urlï¼Œä½¿ç”¨é»˜è®¤çš„OpenAI URL
            if not base_url:
                base_url = "https://api.openai.com/v1"
            
            client = openai.AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=self.default_timeout
            )
            
            logger.info(f"è°ƒç”¨OpenAIå…¼å®¹APIæ¨¡å‹: {model}, æ¶ˆæ¯æ•°é‡: {len(messages)}, åŸºç¡€URL: {base_url}")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - åªæ”¯æŒåŸºæœ¬çš„æ–‡æœ¬æ¶ˆæ¯æ ¼å¼
            converted_messages = []
            for msg in messages:
                role = self._get_message_attr(msg, "role")
                content = self._get_message_attr(msg, "content")
                converted_messages.append({"role": role, "content": content})
            
            # åŸºç¡€å®Œæˆå‚æ•°ï¼ˆOpenAIå…¼å®¹æä¾›å•†åªæ”¯æŒçº¯æ–‡æœ¬å¯¹è¯ï¼‰
            completion_params = {
                "model": model,
                "messages": converted_messages,
                "stream": stream,
                "temperature": 0.7,
                "max_tokens": 4000
            }
            
            response = await asyncio.wait_for(
                client.chat.completions.create(**completion_params),
                timeout=self.default_timeout
            )
            
            result = response.model_dump()
            logger.info(f"OpenAIå…¼å®¹APIè°ƒç”¨æˆåŠŸï¼Œè¿”å›é€‰æ‹©æ•°é‡: {len(result.get('choices', []))}")
            return result
            
        except openai.AuthenticationError as e:
            logger.error(f"OpenAIå…¼å®¹APIè®¤è¯å¤±è´¥: {str(e)}")
            raise Exception("OpenAIå…¼å®¹APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥")
        except openai.RateLimitError as e:
            logger.error(f"OpenAIå…¼å®¹APIé€Ÿç‡é™åˆ¶: {str(e)}")
            raise Exception("OpenAIå…¼å®¹APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•")
        except openai.InternalServerError as e:
            logger.error(f"OpenAIå…¼å®¹APIæœåŠ¡å™¨é”™è¯¯: {str(e)}")
            raise Exception("OpenAIå…¼å®¹APIæœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
        except Exception as e:
            logger.error(f"OpenAIå…¼å®¹APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            raise Exception(f"OpenAIå…¼å®¹APIè°ƒç”¨å¤±è´¥: {str(e)}")

    async def stream_completion(
        self,
        provider: str,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        reasoning: str = "medium",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼å®Œæˆï¼ˆWebSocketä½¿ç”¨ï¼‰"""
        logger.info(f"å¼€å§‹æµå¼è°ƒç”¨ {provider} API")
        
        try:
            if provider == "openai":
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º
                if await self._supports_streaming(provider, model):
                    async for chunk in self._openai_stream_completion(model, messages, api_key, thinking_mode, reasoning_summaries, tools, use_native_search):
                        yield chunk
                else:
                    # ä¸æ”¯æŒæµå¼çš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›å®Œæ•´å“åº”
                    logger.info(f"æ¨¡å‹ {model} ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œä½¿ç”¨æ™®é€šè¯·æ±‚")
                    response = await self.get_completion(provider, model, messages, api_key, False, thinking_mode, reasoning_summaries, reasoning, tools, use_native_search)
                    yield response
            elif provider == "anthropic":
                # Anthropicæ”¯æŒæµå¼è¾“å‡º
                async for chunk in self._anthropic_stream_completion(model, messages, api_key, thinking_mode, reasoning_summaries, tools, use_native_search):
                    yield chunk
            elif provider == "openai_compatible":
                # OpenAIå…¼å®¹æä¾›å•†æ”¯æŒæµå¼
                async for chunk in self._openai_compatible_stream_completion(model, messages, api_key, thinking_mode, reasoning_summaries, tools, use_native_search):
                    yield chunk
            else:
                # å…¶ä»–æä¾›å•†æš‚ä¸æ”¯æŒæµå¼
                response = await self.get_completion(provider, model, messages, api_key, False, thinking_mode, reasoning_summaries, reasoning)
                yield response
                
        except Exception as e:
            logger.error(f"æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            yield {"error": str(e)}

    async def _openai_stream_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """OpenAIæµå¼å®Œæˆ"""
        try:
            client = openai.AsyncOpenAI(
                api_key=api_key,
                timeout=self.default_timeout
            )
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥æ”¯æŒå›¾ç‰‡å’Œæ–‡ä»¶
            converted_messages = [self._convert_message_to_openai_format(msg) for msg in messages]
            
            # å‡†å¤‡å·¥å…·é…ç½®
            tools_config = self._prepare_tools_config(messages, tools)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„å‚æ•°
            stream_params = {
                "model": model,
                "messages": converted_messages,
                "stream": True
            }
            
            # å¤„ç†Web Searchå·¥å…·çš„å›é€€é€»è¾‘
            if tools and use_native_search is False:
                # å¯¹äºä¸æ”¯æŒæ–°ç‰ˆweb_searchçš„æ¨¡å‹ï¼Œä½¿ç”¨æ—§ç‰ˆå›é€€
                stream_params = await self._handle_web_search_fallback(
                    stream_params, tools, messages
                )
            elif tools_config["tools"]:
                stream_params.update(tools_config)
            
            # GPT-5 ç³»åˆ—æ¨¡å‹ä¸æ”¯æŒè‡ªå®šä¹‰ temperatureï¼Œä½¿ç”¨é»˜è®¤å€¼ 1
            if not self._is_gpt5_model(model):
                stream_params["temperature"] = 0.7
            
            # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ max_tokens
            if self._is_gpt5_model(model):
                stream_params["max_completion_tokens"] = 4000
            else:
                stream_params["max_tokens"] = 4000
            
            stream = await client.chat.completions.create(**stream_params)
            
            async for chunk in stream:
                yield chunk.model_dump()
                
        except Exception as e:
            logger.error(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            yield {"error": str(e)}

    async def _openai_compatible_stream_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],  # Support both dict and Pydantic objects
        api_key: str,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None,
        base_url: str = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """OpenAIå…¼å®¹æµå¼å®Œæˆ"""
        try:
            # å¦‚æœæ²¡æœ‰æä¾›base_urlï¼Œä½¿ç”¨é»˜è®¤çš„OpenAI URL
            if not base_url:
                base_url = "https://api.openai.com/v1"
                
            client = openai.AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=self.default_timeout
            )
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - åªæ”¯æŒåŸºæœ¬çš„æ–‡æœ¬æ¶ˆæ¯æ ¼å¼
            converted_messages = []
            for msg in messages:
                role = self._get_message_attr(msg, "role")
                content = self._get_message_attr(msg, "content")
                converted_messages.append({"role": role, "content": content})
            
            # æµå¼å‚æ•°
            stream_params = {
                "model": model,
                "messages": converted_messages,
                "stream": True,
                "temperature": 0.7,
                "max_tokens": 4000
            }
            
            stream = await client.chat.completions.create(**stream_params)
            
            async for chunk in stream:
                yield chunk.model_dump()
                
        except Exception as e:
            logger.error(f"OpenAIå…¼å®¹æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            yield {"error": str(e)}

    async def _anthropic_stream_completion(
        self,
        model: str,
        messages: List[Union[Dict[str, str], Any]],
        api_key: str,
        thinking_mode: bool = False,
        reasoning_summaries: str = "auto",
        tools: List[Dict[str, Any]] = None,
        use_native_search: bool = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Anthropicæµå¼å®Œæˆï¼Œæ”¯æŒExtended Thinkingã€Visionã€Citationsç­‰åŠŸèƒ½"""
        try:
            client = anthropic.AsyncAnthropic(
                api_key=api_key,
                timeout=self.default_timeout
            )
            
            logger.info(f"å¼€å§‹Anthropicæµå¼è°ƒç”¨: {model}, æ‰©å±•æ€è€ƒæ¨¡å¼: {thinking_mode}")
            
            system_message = ""
            user_messages = []
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥æ”¯æŒå¤šåª’ä½“å†…å®¹
            for msg in messages:
                role = self._get_message_attr(msg, "role")
                content = self._get_message_attr(msg, "content")
                
                if role == "system":
                    system_message = content
                else:
                    # è½¬æ¢ä¸ºAnthropic Messages APIæ ¼å¼
                    converted_msg = self._convert_message_to_anthropic_format(msg)
                    user_messages.append(converted_msg)
            
            # æ„å»ºæµå¼è¯·æ±‚å‚æ•°
            stream_params = {
                "model": model,
                "max_tokens": 4000,
                "messages": user_messages,
                "temperature": 0.7,
                "stream": True
            }
            
            # æ·»åŠ systemæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if system_message:
                stream_params["system"] = system_message
            
            # Extended Thinkingæ”¯æŒ
            if thinking_mode:
                stream_params["thinking"] = {
                    "type": "enabled",
                    "budget_tokens": 10000  # å›ºå®šå€¼
                }
                logger.info("å¯ç”¨Claudeæµå¼æ‰©å±•æ€è€ƒæ¨¡å¼ï¼Œbudget_tokens: 10000")
            
            # å·¥å…·é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
            if tools:
                anthropic_tools = self._convert_tools_to_anthropic_format(tools)
                if anthropic_tools:
                    stream_params["tools"] = anthropic_tools
            
            # åˆ›å»ºæµå¼å“åº”
            async with client.messages.stream(**stream_params) as stream:
                content_text = ""
                thinking_content = ""
                message_id = None
                current_citations = []
                
                async for event in stream:
                    try:
                        # æ ¹æ®äº‹ä»¶ç±»å‹å¤„ç†ä¸åŒçš„æµå¼æ•°æ®
                        if hasattr(event, 'type'):
                            if event.type == "message_start":
                                # æ¶ˆæ¯å¼€å§‹
                                if hasattr(event, 'message') and hasattr(event.message, 'id'):
                                    message_id = event.message.id
                                
                                # å‘é€åˆå§‹chunk
                                chunk = {
                                    "id": f"msg_{message_id or 'stream'}",
                                    "choices": [{
                                        "delta": {
                                            "role": "assistant",
                                            "content": ""
                                        },
                                        "index": 0,
                                        "finish_reason": None
                                    }]
                                }
                                yield chunk
                            
                            elif event.type == "content_block_start":
                                # å†…å®¹å—å¼€å§‹ - å¯èƒ½æ˜¯textæˆ–thinking
                                if hasattr(event, 'content_block'):
                                    block_type = getattr(event.content_block, 'type', 'text')
                                    if block_type == "thinking" and thinking_mode:
                                        logger.debug("å¼€å§‹æ¥æ”¶thinkingå†…å®¹")
                            
                            elif event.type == "content_block_delta":
                                # å†…å®¹å¢é‡
                                if hasattr(event, 'delta'):
                                    delta_type = getattr(event.delta, 'type', 'text_delta')
                                    
                                    if delta_type == "text_delta":
                                        # æ–‡æœ¬å¢é‡
                                        text_delta = getattr(event.delta, 'text', '')
                                        content_text += text_delta
                                        
                                        chunk = {
                                            "id": f"msg_{message_id or 'stream'}",
                                            "choices": [{
                                                "delta": {
                                                    "content": text_delta
                                                },
                                                "index": 0,
                                                "finish_reason": None
                                            }]
                                        }
                                        yield chunk
                                    
                                    elif delta_type == "thinking_delta" and thinking_mode:
                                        # æ€è€ƒå¢é‡
                                        thinking_delta = getattr(event.delta, 'content', '') or getattr(event.delta, 'thinking', '')
                                        thinking_content += thinking_delta
                                        
                                        # æ€è€ƒå†…å®¹ä½œä¸ºreasoningå­—æ®µå‘é€
                                        chunk = {
                                            "id": f"msg_{message_id or 'stream'}",
                                            "choices": [{
                                                "delta": {
                                                    "reasoning": thinking_delta
                                                },
                                                "index": 0,
                                                "finish_reason": None
                                            }]
                                        }
                                        yield chunk
                                    
                                    elif delta_type == "citations_delta":
                                        # Citationså¢é‡
                                        if hasattr(event.delta, 'citation'):
                                            citation = {
                                                "type": getattr(event.delta.citation, 'type', 'unknown'),
                                                "cited_text": getattr(event.delta.citation, 'cited_text', ''),
                                                "source": getattr(event.delta.citation, 'source', ''),
                                                "title": getattr(event.delta.citation, 'title', ''),
                                                "document_index": getattr(event.delta.citation, 'document_index', 0)
                                            }
                                            current_citations.append(citation)
                            
                            elif event.type == "message_delta":
                                # æ¶ˆæ¯çº§åˆ«çš„å¢é‡ï¼Œé€šå¸¸åŒ…å«åœæ­¢åŸå› 
                                if hasattr(event, 'delta') and hasattr(event.delta, 'stop_reason'):
                                    stop_reason = self._map_anthropic_stop_reason(event.delta.stop_reason)
                                    
                                    chunk = {
                                        "id": f"msg_{message_id or 'stream'}",
                                        "choices": [{
                                            "delta": {},
                                            "index": 0,
                                            "finish_reason": stop_reason
                                        }]
                                    }
                                    
                                    # å¦‚æœæœ‰citationsï¼Œåœ¨æœ€åçš„chunkä¸­åŒ…å«
                                    if current_citations:
                                        chunk["choices"][0]["delta"]["citations"] = current_citations
                                    
                                    yield chunk
                            
                            elif event.type == "message_stop":
                                # æ¶ˆæ¯ç»“æŸ
                                logger.info(f"Anthropicæµå¼è°ƒç”¨å®Œæˆï¼Œæ€»æ–‡æœ¬é•¿åº¦: {len(content_text)}, thinkingé•¿åº¦: {len(thinking_content)}")
                                break
                    
                    except Exception as e:
                        logger.error(f"å¤„ç†Anthropicæµå¼äº‹ä»¶æ—¶å‡ºé”™: {str(e)}")
                        continue
                        
        except anthropic.AuthenticationError as e:
            logger.error(f"Anthropicæµå¼è®¤è¯å¤±è´¥: {str(e)}")
            yield {"error": "Anthropic APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥"}
        except anthropic.RateLimitError as e:
            logger.error(f"Anthropicæµå¼é€Ÿç‡é™åˆ¶: {str(e)}")
            yield {"error": "Anthropic APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•"}
        except Exception as e:
            logger.error(f"Anthropicæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            yield {"error": f"Anthropicæµå¼è°ƒç”¨å¤±è´¥: {str(e)}"}